"""å†…å®¹æå–å™¨ - æ··åˆç­–ç•¥ï¼šBeautifulSoup æå–æ­£æ–‡ + LLM æå–å…ƒæ•°æ®"""
import json
from typing import Dict, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from rich.console import Console
from bs4 import BeautifulSoup

from config.settings import (
    OPENAI_API_BASE,
    OPENAI_API_KEY,
    MODEL_NAME,
    LLM_TEMPERATURE
)
from utils.helpers import truncate_text, validate_article

console = Console()


class ArticleExtractor:
    """æ–‡ç« å†…å®¹æå–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æå–å™¨"""
        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            model=MODEL_NAME,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            temperature=LLM_TEMPERATURE
        )
        
        # å…ƒæ•°æ®æå–æç¤ºè¯ï¼ˆåªæå–å…ƒæ•°æ®ï¼Œä¸æå–æ­£æ–‡ï¼‰
        self.metadata_extraction_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µå…ƒæ•°æ®æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä» HTML ä¸­æå–æ–‡ç« çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

**æ³¨æ„**ï¼šä½ åªéœ€è¦æå–å…ƒæ•°æ®ï¼Œæ­£æ–‡å†…å®¹ä¼šç”±å…¶ä»–ç³»ç»Ÿå¤„ç†ã€‚

è¯·ä»ä»¥ä¸‹ HTML ä¸­æå–ï¼š

1. **title** (string): æ–‡ç« æ ‡é¢˜ï¼ˆä» <title> æˆ– <h1> ç­‰æ ‡ç­¾æå–ï¼‰
2. **date** (string): å‘å¸ƒæ—¥æœŸï¼ˆISO æ ¼å¼: YYYY-MM-DDï¼Œå¦‚ "2025-12-08"ï¼‰
3. **author** (string|null): ä½œè€…åï¼ˆå¦‚æœæ²¡æœ‰å°±è¿”å› nullï¼‰
4. **categories** (array): åˆ†ç±»/æ ‡ç­¾åˆ—è¡¨ï¼ˆå¦‚ ["Press Releases", "News"]ï¼‰

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰**ï¼š
```json
{{
  "title": "æ–‡ç« æ ‡é¢˜",
  "date": "2025-12-08",
  "author": "ä½œè€…å",
  "categories": ["åˆ†ç±»1", "åˆ†ç±»2"]
}}
```

**æ³¨æ„**ï¼š
- å¦‚æœæŸä¸ªå­—æ®µæ‰¾ä¸åˆ°ï¼Œç”¨ null æˆ–ç©ºæ•°ç»„
- æ—¥æœŸå°½é‡è½¬æ¢ä¸º YYYY-MM-DD æ ¼å¼
- æ ‡é¢˜è¦å»é™¤ç½‘ç«™åç§°ç­‰åç¼€

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""),
            ("user", "URL: {url}\n\nHTMLï¼ˆå‰30000å­—ç¬¦ï¼‰:\n{html}")
        ])
        
    def _extract_content_with_beautifulsoup(self, html: str) -> str:
        """
        ä½¿ç”¨ BeautifulSoup æå–æ­£æ–‡å†…å®¹
        
        Args:
            html: HTML å†…å®¹
            
        Returns:
            str: æ­£æ–‡å†…å®¹
        """
        soup = BeautifulSoup(html, 'lxml')
        
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾ï¼ˆåœ¨æŸ¥æ‰¾æ­£æ–‡ä¹‹å‰ï¼‰
        for tag in soup.select('script, style, nav, header, footer, aside, iframe, noscript, .sidebar, .navigation, .menu, .comments'):
            tag.decompose()
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
        content_selectors = [
            'article .entry-content',
            '.entry-content',
            'article .post-content',
            '.post-content',
            'article .article-content',
            '.article-content',
            'article',  # article æ ‡ç­¾æœ¬èº«é€šå¸¸å°±æ˜¯æ­£æ–‡
            '.content',
            'main article',
            'main',
        ]
        
        content_elem = None
        for selector in content_selectors:
            try:
                elem = soup.select_one(selector)
                if elem:
                    # åœ¨é€‰ä¸­çš„å…ƒç´ å†…å†æ¬¡ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
                    for unwanted in elem.select('.related, .share, .social, .advertisement, .ad, .author-box, .tags'):
                        unwanted.decompose()
                    
                    # æ£€æŸ¥å†…å®¹é•¿åº¦æ˜¯å¦åˆç†
                    text = elem.get_text(strip=True, separator='\n')
                    if len(text) > 200:  # è‡³å°‘200å­—ç¬¦æ‰è®¤ä¸ºæ˜¯æ­£æ–‡
                        content_elem = elem
                        console.log(f"[cyan]    ä½¿ç”¨é€‰æ‹©å™¨: {selector}, é•¿åº¦: {len(text)}[/cyan]")
                        break
            except Exception as e:
                console.log(f"[yellow]    é€‰æ‹©å™¨ {selector} é”™è¯¯: {e}[/yellow]")
                continue
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå›é€€åˆ° body
        if not content_elem:
            console.log("[yellow]    æœªæ‰¾åˆ°åˆé€‚çš„å†…å®¹å…ƒç´ ï¼Œä½¿ç”¨ body[/yellow]")
            content_elem = soup.find('body') or soup
        
        # æå–æ–‡æœ¬ï¼Œä¿ç•™æ®µè½ç»“æ„
        paragraphs = []
        
        # ä¼˜å…ˆæå– p, h1-h6, li æ ‡ç­¾çš„å†…å®¹
        for tag in content_elem.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
            text = tag.get_text(strip=True)
            # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹å’Œå¸¸è§çš„å¯¼èˆªæ–‡æœ¬
            if text and len(text) > 15:
                # è·³è¿‡æ˜æ˜¾çš„å¯¼èˆªæˆ–èœå•é¡¹
                lower_text = text.lower()
                if any(skip in lower_text for skip in ['click here', 'read more', 'learn more', 'subscribe', 'follow us']):
                    if len(text) < 50:  # çŸ­çš„å¯¼èˆªæ–‡æœ¬ç›´æ¥è·³è¿‡
                        continue
                paragraphs.append(text)
        
        # å¦‚æœæ²¡æ‰¾åˆ°æ®µè½ï¼Œç›´æ¥è·å–æ‰€æœ‰æ–‡æœ¬
        if not paragraphs or len('\n\n'.join(paragraphs)) < 100:
            console.log("[yellow]    æ®µè½æå–å¤±è´¥ï¼Œä½¿ç”¨æ•´ä½“æ–‡æœ¬[/yellow]")
            return content_elem.get_text(strip=True, separator='\n\n')
        
        result = '\n\n'.join(paragraphs)
        console.log(f"[green]    æå–äº† {len(paragraphs)} ä¸ªæ®µè½ï¼Œæ€»é•¿åº¦: {len(result)} å­—ç¬¦[/green]")
        return result
    
    def extract_article(self, url: str, html: str) -> Optional[Dict]:
        """
        æå–æ–‡ç« å†…å®¹ï¼ˆæ··åˆç­–ç•¥ï¼‰
        
        Args:
            url: æ–‡ç«  URL
            html: HTML å†…å®¹
            
        Returns:
            Dict: æ–‡ç« æ•°æ®ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        console.log(f"[cyan]ğŸ“„ æå–æ–‡ç« : {url}[/cyan]")
        
        try:
            # æ­¥éª¤ 1: ä½¿ç”¨ BeautifulSoup æå–æ­£æ–‡
            console.log("[cyan]  1/2 ä½¿ç”¨ BeautifulSoup æå–æ­£æ–‡...[/cyan]")
            content = self._extract_content_with_beautifulsoup(html)
            
            if len(content) < 100:
                console.log("[yellow]âš ï¸  æå–çš„æ­£æ–‡å¤ªçŸ­ï¼Œå¯èƒ½å¤±è´¥[/yellow]")
            
            # æ­¥éª¤ 2: ä½¿ç”¨ LLM æå–å…ƒæ•°æ®
            console.log("[cyan]  2/2 ä½¿ç”¨ LLM æå–å…ƒæ•°æ®...[/cyan]")
            html_truncated = truncate_text(html, 30000)
            
            messages = self.metadata_extraction_prompt.format_messages(
                url=url,
                html=html_truncated
            )
            
            response = self.llm.invoke(messages)
            response_content = response.content.strip()
            
            # æå– JSON
            if '```json' in response_content:
                response_content = response_content.split('```json')[1].split('```')[0].strip()
            elif '```' in response_content:
                response_content = response_content.split('```')[1].split('```')[0].strip()
                
            # è§£æ JSON
            metadata = json.loads(response_content)
            
            # ç»„åˆæ•°æ®
            article = {
                'title': metadata.get('title', ''),
                'date': metadata.get('date'),
                'author': metadata.get('author'),
                'categories': metadata.get('categories', []),
                'content': content,  # ä½¿ç”¨ BeautifulSoup æå–çš„åŸå§‹æ­£æ–‡
                'url': url
            }
            
            # ç”Ÿæˆæ‘˜è¦ï¼ˆå–æ­£æ–‡å‰200å­—ç¬¦ï¼‰
            article['summary'] = content[:200] + '...' if len(content) > 200 else content
            
            # éªŒè¯æ•°æ®è´¨é‡
            if not validate_article(article):
                console.log("[red]âœ— æ–‡ç« æ•°æ®ä¸å®Œæ•´æˆ–è´¨é‡ä¸ä½³[/red]")
                return None
                
            content_length = len(article.get('content', ''))
            console.log(f"[green]âœ“ æå–æˆåŠŸ: {article['title'][:50]}... ({content_length} å­—ç¬¦)[/green]")
            
            return article
            
        except json.JSONDecodeError as e:
            console.log(f"[red]âœ— JSON è§£æå¤±è´¥: {e}[/red]")
            console.log(f"[yellow]åŸå§‹å“åº”: {response_content[:500] if 'response_content' in locals() else 'N/A'}[/yellow]")
            return None
            
        except Exception as e:
            console.log(f"[red]âœ— æå–å¤±è´¥: {e}[/red]")
            import traceback
            console.log(f"[red]{traceback.format_exc()}[/red]")
            return None
            
    def extract_batch(self, urls_and_htmls: list) -> list:
        """
        æ‰¹é‡æå–æ–‡ç« 
        
        Args:
            urls_and_htmls: [(url, html), ...] åˆ—è¡¨
            
        Returns:
            list: æ–‡ç« åˆ—è¡¨
        """
        articles = []
        total = len(urls_and_htmls)
        
        console.log(f"[cyan]ğŸ“¦ æ‰¹é‡æå– {total} ç¯‡æ–‡ç« ...[/cyan]")
        
        for i, (url, html) in enumerate(urls_and_htmls, 1):
            console.log(f"\n[bold cyan][{i}/{total}][/bold cyan]")
            article = self.extract_article(url, html)
            if article:
                articles.append(article)
                
        console.log(f"\n[green]âœ“ æˆåŠŸæå– {len(articles)}/{total} ç¯‡æ–‡ç« [/green]")
        return articles
