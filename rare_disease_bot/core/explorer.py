"""ç½‘ç«™ç»“æ„æ¢ç´¢å™¨ - ä½¿ç”¨ Qwen3-max åˆ†æé¡µé¢ç»“æ„"""
import json
from typing import Dict, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from rich.console import Console

from config.settings import (
    OPENAI_API_BASE,
    OPENAI_API_KEY,
    MODEL_NAME,
    LLM_TEMPERATURE
)
from utils.helpers import clean_html, truncate_text

console = Console()


class WebsiteExplorer:
    """ç½‘ç«™ç»“æ„æ¢ç´¢å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¢ç´¢å™¨"""
        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            model=MODEL_NAME,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            temperature=LLM_TEMPERATURE
        )
        
        # åˆ†ææç¤ºè¯
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªç½‘é¡µç»“æ„åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æ HTML é¡µé¢ï¼Œè¯†åˆ«é¡µé¢ç±»å‹å’Œç»“æ„ã€‚

è¯·åˆ†æä»¥ä¸‹ HTML å¹¶æå–ä¿¡æ¯ï¼š

1. **é¡µé¢ç±»å‹**ï¼š
   - article_list: æ–‡ç« åˆ—è¡¨é¡µï¼ˆåŒ…å«å¤šä¸ªæ–‡ç« é“¾æ¥ï¼‰
   - single_article: å•ç¯‡æ–‡ç« é¡µ
   - blog_home: åšå®¢é¦–é¡µ
   - news_archive: æ–°é—»å½’æ¡£é¡µ

2. **æ–‡ç« é“¾æ¥ä½ç½®**ï¼š
   - ä»”ç»†æŸ¥æ‰¾ HTML ä¸­çœŸæ­£çš„æ–‡ç« æ ‡é¢˜é“¾æ¥
   - å¸¸è§æ¨¡å¼ï¼šarticle h4 a, article h3 a, h4 a, h3 a, h2 a
   - æ’é™¤å¯¼èˆªèœå•ã€é¡µçœ‰é¡µè„šä¸­çš„é“¾æ¥
   - é€‰æ‹©å™¨ç¤ºä¾‹: "article h4 a", "h4 a", ".post-title a", ".news-item a"

3. **åˆ†é¡µä¿¡æ¯**ï¼š
   - æ˜¯å¦æœ‰åˆ†é¡µ
   - æ€»é¡µæ•°ï¼ˆå¦‚æœèƒ½è¯†åˆ«ï¼‰
   - ä¸‹ä¸€é¡µçš„é€‰æ‹©å™¨

4. **å…¶ä»–ä¿¡æ¯**ï¼š
   - æ–‡ç« æ•°é‡
   - é¡µé¢å¸ƒå±€ç‰¹ç‚¹

**é‡è¦æç¤º**ï¼š
- ä¼˜å…ˆæŸ¥æ‰¾ h2, h3, h4 æ ‡ç­¾å†…çš„é“¾æ¥ï¼Œè¿™äº›é€šå¸¸æ˜¯æ–‡ç« æ ‡é¢˜
- å¦‚æœå‘ç° article h4 a æˆ– h4 a æœ‰å¤šä¸ªé“¾æ¥ï¼Œä¼˜å…ˆä½¿ç”¨å®ƒä»¬
- ä¸è¦é€‰æ‹©å¯¼èˆªèœå•ä¸­çš„é“¾æ¥

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰**ï¼š
```json
{{
  "page_type": "article_list",
  "article_count": 10,
  "selectors": {{
    "article_links": "article h4 a",
    "article_titles": "article h4",
    "next_page": "a.next"
  }},
  "pagination": {{
    "has_pagination": true,
    "total_pages": 108,
    "pagination_selector": ".pagination"
  }},
  "notes": "é¡µé¢åŒ…å«æ–‡ç« åˆ—è¡¨ï¼Œæ¯é¡µ10ç¯‡æ–‡ç« ï¼Œæ–‡ç« æ ‡é¢˜åœ¨ h4 æ ‡ç­¾å†…"
}}
```

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""),
            ("user", "URL: {url}\n\nHTMLï¼ˆå‰30000å­—ç¬¦ï¼‰:\n{html}")
        ])
        
    def analyze_page_structure(self, url: str, html: str) -> Dict:
        """
        åˆ†æé¡µé¢ç»“æ„
        
        Args:
            url: é¡µé¢ URL
            html: HTML å†…å®¹
            
        Returns:
            Dict: é¡µé¢ç»“æ„ä¿¡æ¯
        """
        console.log("[cyan]ğŸ” åˆ†æé¡µé¢ç»“æ„...[/cyan]")
        
        try:
            # æˆªæ–­ HTMLï¼ˆé¿å…è¶…è¿‡ token é™åˆ¶ï¼‰
            html_truncated = truncate_text(html, 30000)
            
            # è°ƒç”¨ LLM åˆ†æ
            messages = self.analysis_prompt.format_messages(
                url=url,
                html=html_truncated
            )
            
            response = self.llm.invoke(messages)
            content = response.content.strip()
            
            # æå– JSON
            # æœ‰æ—¶ LLM ä¼šç”¨ ```json åŒ…è£¹ï¼Œéœ€è¦æ¸…ç†
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0].strip()
            elif '```' in content:
                content = content.split('```')[1].split('```')[0].strip()
                
            # è§£æ JSON
            result = json.loads(content)
            
            console.log(f"[green]âœ“ é¡µé¢ç±»å‹: {result.get('page_type')}[/green]")
            console.log(f"[green]âœ“ æ–‡ç« æ•°é‡: {result.get('article_count')}[/green]")
            
            if result.get('pagination', {}).get('has_pagination'):
                total = result['pagination'].get('total_pages', 'æœªçŸ¥')
                console.log(f"[green]âœ“ åˆ†é¡µ: æ˜¯ (å…± {total} é¡µ)[/green]")
            
            return result
            
        except json.JSONDecodeError as e:
            console.log(f"[red]âœ— JSON è§£æå¤±è´¥: {e}[/red]")
            console.log(f"[yellow]åŸå§‹å“åº”: {content[:500]}[/yellow]")
            return self._fallback_analysis(html)
            
        except Exception as e:
            console.log(f"[red]âœ— åˆ†æå¤±è´¥: {e}[/red]")
            return self._fallback_analysis(html)
            
    def _fallback_analysis(self, html: str) -> Dict:
        """å›é€€åˆ†æï¼ˆç®€å•è§„åˆ™ï¼‰"""
        console.log("[yellow]âš ï¸  ä½¿ç”¨å›é€€åˆ†æç­–ç•¥[/yellow]")
        
        # ç®€å•çš„å¯å‘å¼åˆ†æ
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'lxml')
        
        # æŸ¥æ‰¾å¸¸è§çš„æ–‡ç« é“¾æ¥
        article_links = []
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        selectors = [
            'article h4 a',  # rarediseases.org ä½¿ç”¨è¿™ä¸ª
            'h4 a',
            'article h3 a',
            'h3 a',
            'article h2 a',
            'h2 a',
            '.post-title a',
            '.entry-title a',
            'article a',
            '.post a',
            '.news-item a',
            '.article-title a'
        ]
        
        best_selector = None
        max_links = 0
        best_links = []
        
        for selector in selectors:
            try:
                links = soup.select(selector)
                # è¿‡æ»¤æ‰å¯¼èˆªé“¾æ¥ç­‰ï¼Œåªä¿ç•™çœŸæ­£çš„æ–‡ç« é“¾æ¥
                valid_links = [
                    link for link in links 
                    if link.get('href') and 
                    ('http' in link.get('href') or link.get('href').startswith('/')) and
                    link.text.strip()  # å¿…é¡»æœ‰æ–‡æœ¬
                ]
                
                # é€‰æ‹©æ•°é‡åˆç†çš„é€‰æ‹©å™¨ï¼ˆé€šå¸¸æ–‡ç« åˆ—è¡¨æœ‰5-20ç¯‡ï¼‰
                if 5 <= len(valid_links) <= 50 and len(valid_links) > max_links:
                    max_links = len(valid_links)
                    best_selector = selector
                    best_links = valid_links
            except:
                pass
        
        # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„ï¼Œæ”¾å®½æ¡ä»¶
        if not best_selector:
            for selector in selectors:
                try:
                    links = soup.select(selector)
                    if len(links) > max_links:
                        max_links = len(links)
                        best_selector = selector
                except:
                    pass
                
        # æ£€æµ‹åˆ†é¡µ
        has_pagination = bool(soup.select('.pagination, .pager, .page-numbers'))
        
        console.log(f"[cyan]å›é€€åˆ†æ: é€‰æ‹©å™¨='{best_selector}', é“¾æ¥æ•°={max_links}[/cyan]")
        
        return {
            'page_type': 'article_list' if max_links > 3 else 'single_article',
            'article_count': max_links,
            'selectors': {
                'article_links': best_selector or 'a'
            },
            'pagination': {
                'has_pagination': has_pagination,
                'total_pages': None
            },
            'notes': 'ä½¿ç”¨å›é€€åˆ†æ'
        }
